{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自然语言处理相关库\n",
    "  * SnowNLP\n",
    "  * THULAC\n",
    "  * 小明nlp\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SnowNLP\n",
    "        SnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，由于现在大部分的自然语言处理库基本都是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。注意本程序都是处理的unicode编码，所以使用时请自行decode成unicode。\n",
    "    <https://github.com/isnowfy/snownlp><br>\n",
    "\n",
    "    中文分词（Character-Based Generative Model）\n",
    "    词性标注（TnT 3-gram 隐马）\n",
    "    情感分析（现在训练数据主要是买卖东西时的评价，所以对其他的一些可能效果不是很好，待解决）\n",
    "    文本分类（Naive Bayes）\n",
    "    转换成拼音（Trie树实现的最大匹配）\n",
    "    繁体转简体（Trie树实现的最大匹配）|\n",
    "    提取文本关键词（TextRank算法）\n",
    "    提取文本摘要（TextRank算法）\n",
    "    tf，idf\n",
    "    Tokenization（分割成句子）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['语言', '自然', '计算机']\n",
      "['因而它是计算机科学的一部分', '自然语言处理是计算机科学领域与人工智能领域中的一个重要方向', '自然语言处理是一门融语言学、计算机科学、数学于一体的科学']\n",
      "['自然语言处理是计算机科学领域与人工智能领域中的一个重要方向', '它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法', '自然语言处理是一门融语言学、计算机科学、数学于一体的科学', '因此', '这一领域的研究将涉及自然语言', '即人们日常使用的语言', '所以它与语言学的研究有着密切的联系', '但又有重要的区别', '自然语言处理并不是一般地研究自然语言', '而在于研制能有效地实现自然语言通信的计算机系统', '特别是其中的软件系统', '因而它是计算机科学的一部分']\n"
     ]
    }
   ],
   "source": [
    "from snownlp import SnowNLP\n",
    "text = u'''\n",
    "自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。\n",
    "它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。\n",
    "自然语言处理是一门融语言学、计算机科学、数学于一体的科学。\n",
    "因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，\n",
    "所以它与语言学的研究有着密切的联系，但又有重要的区别。\n",
    "自然语言处理并不是一般地研究自然语言，\n",
    "而在于研制能有效地实现自然语言通信的计算机系统，\n",
    "特别是其中的软件系统。因而它是计算机科学的一部分。\n",
    "'''\n",
    "\n",
    "s = SnowNLP(text)\n",
    "\n",
    "print(s.keywords(3)) #关键词\n",
    "print(s.summary(3))  #摘要\n",
    "print(s.sentences)   #提取句子\n",
    "# print(s.pinyin)      #拼音\n",
    "zip_tag = s.tags\n",
    "# for i in zip_tag:\n",
    "#     print(i)        #词性标注\n",
    "# print(s.tf)\n",
    "# print(s.idf)\n",
    "# print(s.sim([u'自然']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THULAC \n",
    "\n",
    "    THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点：\n",
    "        能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。\n",
    "        准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。\n",
    "        速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s\n",
    "   <http://thulac.thunlp.org/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n",
      "我_r 爱_v 北京_ns 天安门_ns\n"
     ]
    }
   ],
   "source": [
    "import thulac\n",
    "\n",
    "thu1 = thulac.thulac()  #默认模式\n",
    "text = thu1.cut(\"我爱北京天安门\", text=True)  #进行一句话分词\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
